# Phase I: Analyzing Users, Competitors, and Initial Designs

## Introduction

TCG_Binder aims to provide a platform where trading card game players can effortlessly manage their physical trading card collections digitally. The focus during the first phase of the project has been to define our target market as well as to gain an understanding of the current products and competitors. We were able to accomplish these goals by conducting competitive analysis and heuristic evaluation of the target market and the products that exist in the space. 

## Methods

Two main research methods were used in discovering new insights: **Competitive Analysis** and **Heuristic Evaluation**.

Our <ins>competitive analysis</ins> was performed by having one member of the evaluation team individually choose an app or piece of software of similar functionality to the app being developed by our software engineering team and determining its **strengths, weaknesses, quality level, price/cost**, and **platform**. The purpose of performing a competitive analysis was to discover what the use experience was like while using one of these applications. By doing this, we were able to not only determine how the app felt individually, but see what the community surrounding these apps thought about existing features. 

In total, we evaluated four apps: [Ludex](https://www.ludex.com/) - a sports card and TCG scanner, [TCG Player](https://play.google.com/store/apps/details?id=com.tcgplayer.tcgplayer&hl=en_US&gl=US&pli=1) - a scanner for YuGiOh, Pokemon, and Magic The Gathering cards, [MTG Scanner](https://play.google.com/store/apps/details?id=pt.tscg.mtgmanager&hl=en_US&gl=US) - a scanner specifically for Magic the Gathering, and [ManaBox](https://www.manabox.app/) - a mobile MTG deck builder. 

As mentioned, each evaluator performed an analysis on the apps’ functionality in order to figure out its strengths and weaknesses. Traversing the UI, testing out the various functions, and scanning the reviews were the main methods in determining the **strengths, weaknesses, quality level, price/cost**, and **platform**. 

Our <ins>heuristic analysis</ins> utilized a total of [ten heuristics](https://www.nngroup.com/articles/ten-usability-heuristics/). Each evaluator performed a heuristic analysis on a different app using these heuristics. Heuristic analysis offers a more precise examination of a piece of software by focusing on a more specific aspect of the application. 

**Visibility of System Status** examines the quality of an apps’ feedback to the user when interacting with the UI. Good systems are able to effectively communicate to the user what the system’s state is such that they understand what to do next. Likewise, feedback should be presented to the user as quickly as possible.

**Match Between System and the Real World** focuses on how well the design presents itself in a familiar way to the user. For example, the usual convention for interacting with a button is to tap or press it. If an application had all of their buttons require a swipe to activate, most users would likely feel unfamiliar or uncomfortable with how they interact with the system. 

**User Control and Freedom** explores how well a system presents its actions in a clear and succinct fashion such that users feel in control of the actions they chose to take. Having an exit button on your app that doesn’t exit can make users feel trapped and not in control of the system. 

**Consistency and Standards** examines if the app follows platform and industry conventions such that using the app feels consistent and familiar to experiences using other apps. Digital interactions with different applications, especially on mobile devices, usually feel similar because they utilize industry and platform conventions. 

**Error Prevention** dissects how an application handles unforeseen events. Users can make logical mistakes: clicking a button by accident. Mismatches between a user’s mental model and the design with no way to undo or change the result speaks to poor error prevention design. 

**Recognition Rather than Recall** looks into how well an app makes its elements, actions, and options easily recognizable/familiar to users. Good design limits the amount of information a user has to recall to navigate the system. 

**Flexibility and Efficiency** of Use examines how well a system can provide different avenues of navigation/use. For example, a new user may start by using buttons given to perform actions while a veteran user will pick up on keyboard shortcuts for the same actions to expedite their work.

**Aesthetic and Minimalist Design** explores how well content and visual design is handled. Applications that are too busy can distract users from more critical information that they would need to perform desired tasks. 

**Help Users Recognize, Diagnose, and Recover from Errors** looks into the quality of error messages and error recovery. Offering users easy to understand error messages can help them better search for solutions to the problem. Using code names or jargon is bad practice. 

**Help and Documentation** should be easy to find and concise. Offering users quick and effective ways to resolve problems they have is good practice. Not offering any documentation or ways to reach out for help can leave users feeling frustrated and lost.

## Findings

Some insights gained after completing our <ins>competitive analysis</ins> were that many of the applications in the market we’re developing within have, at bare minimum, card scanning, pricing, and search filtering. Having these features in our own application would be ideal as seems the convention for TCG scanners across the market. Alongside that, experiencing how the UI felt to navigate, a better understanding of what a good user design layout would look for our own application. 

Some insights gained after completing our <ins>heuristic analysis</ins> were that all of the apps in our evaluation handled at least a few aspects of each heuristic. Not one app failed to demonstrate at least one feature or function that fell within one of the ten heuristics. Some discoveries made about user needs were that users needs from these apps are: consistent and fast scanners, simple and easy to navigate UI, and error-proof systems. 

## Conclusions

From our competitive and heuristic analysis, we discovered what good systems for this archetype of application. These systems offered *minimalistic design, efficient navigation, fleshed-out & functional features*, and *largely error-proof systems*. These key takeaways influence how we will develop our own application. In terms of UX design, a focus on developing features that are easy to use, efficient, error-proof, and fleshed-out will be a priority. More specifically tuned to scanner based applications, having a scanner that offers quick user feedback is important. Overall, the design of our application should be focused on fulfilling these design principles in the future.  

## Caveats

There were various caveats to the methods we implemented. The biggest caveat was not having access to actual users to query or test with. As a result, our conclusions of what we believe to be a good design or what potential user needs are may not be accurate. Our personas and scenarios are a good educated pitch to envision how our product will be used and valued, but it is not a concrete confirmation. If we had access to actual users, we could get a much more accurate and clear understanding of what our users really want. Another caveat was not having a budget for implementing our research methods. As a result, the research may not be as thorough or in-depth as industry standards would hope it to be. If we had a budget, then we could utilize far more resources, such as implementing other research methods that require money to help bolster and broaden our research findings.
